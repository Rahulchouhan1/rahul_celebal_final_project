# ğŸš€ Azure Data Factory: Conditional ETL Pipeline â€“ Customer & Product Data

## ğŸ“Œ Problem Statement

Design a **dynamic and conditional data pipeline** in **Azure Data Factory (ADF)** that:

1. ğŸ’¼ Copies **Customer Data** from a database to **Azure Data Lake Storage Gen2 (ADLS)** only if the number of customer records is **greater than 500**.
2. ğŸ”„ If the condition is met, the pipeline triggers a **child pipeline** that:
   - Copies **Product Data** to ADLS
   - But **only if customer count > 600**
3. ğŸ§® Customer count must be passed from the parent pipeline to the child pipeline via **pipeline parameters**.

---

## ğŸ§  Logic & Design Architecture

### ğŸ“ Parent Pipeline Responsibilities

- ğŸ” **Lookup Activity**
  - Fetch customer count from source database

- ğŸ“¥ **Set Variable**
  - Store the fetched count

- ğŸ§® **If Condition**
  - Check if `customerCount > 500`
  - If true:
    - ğŸ› ï¸ **Copy Data**: Move customer data to ADLS
    - ğŸš€ **Execute Pipeline**: Trigger child pipeline, passing `customerCount` as a parameter

### ğŸ“ Child Pipeline Responsibilities

- ğŸ“¤ Accepts `customerCount` as a **parameter**
- âœ… **If Condition** checks if `customerCount > 600`
- ğŸ› ï¸ **Copy Data** moves **Product Data** to ADLS if condition is satisfied

---

## ğŸ§± Key ADF Components Used

| Component            | Purpose                                                  |
|---------------------|-----------------------------------------------------------|
| ğŸ” Lookup Activity   | Retrieves customer count from the source table            |
| ğŸ“¥ Set Variable      | Stores retrieved count in a pipeline-level variable       |
| ğŸ§® If Condition      | Applies logic to conditionally execute activities         |
| ğŸ“‚ Copy Data         | Transfers data to Azure Data Lake Storage Gen2            |
| ğŸš€ Execute Pipeline  | Triggers child pipeline and passes parameters             |
| ğŸ¯ Pipeline Parameter| Enables inter-pipeline communication with dynamic values  |

---

## ğŸ—ï¸ Step-by-Step Pipeline Creation

---

### ğŸ”· Step 1: Create the Parent Pipeline (`CopyCustomerData_Pipeline`)

#### ğŸ”¹ 1. Lookup Activity
- **Purpose**: Retrieve customer count
- **Query**:
  ```sql
  SELECT COUNT(*) as totalCount FROM Customer
  ```

#### ğŸ”¹ 2. Set Variable
- **Purpose**: Store the count from lookup
- **Value Expression**:
  ```json
  @activity('GetCustomerCount').output.firstRow.totalCount
  ```

#### ğŸ”¹ 3. If Condition â€“ `CustomerCount > 500`
- **Expression**:
  ```json
  @greater(variables('CustomerCount'), 500)
  ```

âœ… Inside the True block:
- ğŸ—ƒï¸ **Copy Data** â†’ Copy **Customer** table to ADLS  
- ğŸš€ **Execute Pipeline** â†’ Call `ChildPipeline_ProductCopy`  
  - Pass parameter:
    ```json
    @variables('CustomerCount')
    ```

---

### ğŸ”· Step 2: Create the Child Pipeline (`ChildPipeline_ProductCopy`)

#### ğŸ”¹ 1. Add Pipeline Parameter
- **Name**: `customerCount`
- **Type**: `Int`

#### ğŸ”¹ 2. If Condition â€“ `CustomerCount > 600`
- **Expression**:
  ```json
  @greater(int(pipeline().parameters.customerCount), 600)
  ```

âœ… Inside the True block:
- ğŸ—ƒï¸ **Copy Data** â†’ Copy **Product** table to ADLS

---

### ğŸ”· Step 3: Export and Deploy

- ğŸ“¤ **Export ARM Template**:  
  Go to **Manage > ARM Template > Export**

- ğŸ—‚ï¸ **Upload to GitHub**:  
  Place exported templates in:  
  ```
  /arm_templates/
  ```

---
## âœ… Sample Output (from Execution)

---

### ğŸŸ¢ Parent Pipeline â€“ `CopyCustomerData_Pipeline`

- **Customer Count Retrieved**: `1000`
- **CopyCustomerData Activity** â†’ âœ… **Succeeded**
  - Rows Copied: `1000`
- **ExecutePipeline Activity** â†’ âœ… **Child Pipeline Triggered Successfully**

---

### ğŸŸ¢ Child Pipeline â€“ `ChildPipeline_ProductCopy`

- **Parameter Received**: `customerCount = 1000`
- **Condition (customerCount > 600)** â†’ âœ… **True**
- **CopyProductData Activity** â†’ âœ… **Succeeded**
  - Rows Copied: `1000`

---
## ğŸ“ Repository Structure

```bash
ADF-Customer-Product-Pipeline/
â”œâ”€â”€ arm_templates/                      # Deployment artifacts for Azure
â”‚   â”œâ”€â”€ factory/
â”‚   â”‚   â”œâ”€â”€ CopyCustomerData_Pipeline.json       # Parent pipeline definition
â”‚   â”‚   â”œâ”€â”€ ChildPipeline_ProductCopy.json       # Child pipeline definition
â”‚   â”œâ”€â”€ linkedTemplates/                         # Any nested ARM templates (if applicable)
â”‚   â”œâ”€â”€ ARMTemplateForFactory.json               # Factory-wide ARM export
â”‚   â””â”€â”€ ARMTemplateParametersForFactory.json     # Parameters for deployment
â”œâ”€â”€ sample_data/                       # Sample input data
â”‚   â”œâ”€â”€ mock_customer_data.csv
â”‚   â””â”€â”€ mock_product_data.csv
â”œâ”€â”€ screenshots/                       # Visual references
â”‚   â”œâ”€â”€ pipeline_flow.png
â”‚   â””â”€â”€ output_success.png
â””â”€â”€ README.md                          # Project overview and instructions
```

---

## ğŸš€ Project Overview

This project contains a **dynamic conditional pipeline** architecture:

- âœ… **Parent Pipeline**: Copies customer data **only if record count > 500**
- âœ… **Child Pipeline**: Copies product data **only if customer count > 600**
- âœ… Count passed as parameter between pipelines
- ğŸ“‚ Destination: Azure Data Lake Storage Gen2

---

## ğŸ“„ Files of Interest

| File                                     | Purpose                                      |
|------------------------------------------|----------------------------------------------|
| `CopyCustomerData_Pipeline.json`         | Implements parent flow with lookup, condition, and execute |
| `ChildPipeline_ProductCopy.json`         | Implements child flow with param check and copy |
| `ARMTemplateForFactory.json`             | Full export of ADF pipeline setup            |
| `ARMTemplateParametersForFactory.json`   | Parameter file for deployment flexibility    |
| `mock_customer_data.csv`                 | Sample customer data to test flow            |
| `mock_product_data.csv`                  | Sample product data for conditional copy     |
| `pipeline_flow.png`                      | Visual overview of pipeline logic            |
| `output_success.png`                     | Screenshot of successful debug output        |

---

## ğŸ“¦ Deployment Instructions

1. Navigate to the Azure Data Factory â†’ **Manage** section
2. Import `ARMTemplateForFactory.json` and parameters JSON
3. Validate linked services, datasets, and triggers
4. Publish the pipeline and test with sample data

---

## ğŸ§ª How to Use

---

### âš™ï¸ Setup Instructions

1. **Import ARM Templates**  
   Navigate to your Azure Data Factory instance and import the following:
   - `ARMTemplateForFactory.json`
   - `ARMTemplateParametersForFactory.json`

2. **Configure Linked Services**  
   - Set up connections to:
     - ğŸ“˜ Source SQL Database
     - ğŸ—‚ï¸ Azure Data Lake Storage Gen2 (ADLS)

3. **Trigger the Pipeline**  
   - Run the **Parent Pipeline** manually or via a scheduled trigger.

4. **Monitor Execution**  
   - Use the **Monitor tab** to validate activity status and debug results.

---

## ğŸ“ˆ Future Enhancements

- ğŸ“ Add logging to **ADLS folders or SQL audit tables** for traceability
- ğŸ“£ Integrate **email/SMS alerts** using Logic Apps for failure notifications
- ğŸ—ƒï¸ Add **dynamic file naming** using `utcnow()` timestamp or `GUID` functions
- ğŸ§© Parameterize **SQL connection strings** and **ADLS paths** for flexible reuse
- ğŸ”’ Add retry policies and fault tolerance using **Until** and **Wait** activities

---

## ğŸ”— Author & Links

- ğŸ‘¨â€ğŸ’» Built by **Rahul Singh Chouhan**
- ğŸ”— GitHub Repository: [github.com/your-username/ADF-Customer-Product-Pipeline](https://github.com/your-username/ADF-Customer-Product-Pipeline)




